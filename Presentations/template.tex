\documentclass{article}


\usepackage{arxiv}
\usepackage{setspace}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage[section]{placeins}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true,citecolor=black,linkcolor=black,urlcolor=blue]{hyperref}
\usepackage{tikz,graphics,color,fullpage,float,epsf,caption,subcaption}


\title{230T-2 REPORT -- FORECASTING ECONOMIC AND FINANCIAL TIME SERIES: ARIMA VS. LSTM}


\author{
 Upamanyu Pathare \\
  %% examples of more authors
   \And
 Yutao Shu \\
  
  \And
 Bolun Deng \\
 
  \And
 Yongsheng Liu \\
 
  \And
 Hexin Xu \\
  
}

\begin{document}

\maketitle
\begin{abstract}
 Forecasting time series data is an important subject in economics, business, and finance. 
 The paper
 \textit{Forecasting Economic and Financial Time Series: ARIMA vs. LSTM}
 advocates the benefits of applying deep learning-based algorithms to the economics and financial data. Our work in this report is evaluating the merits of the paper’s comparative approach to predicting financial time series using LSTM vs. ARIMA models by applying both models on common financial time series. We will also evaluate alternative deep-learning models on more fine-grained data.
 
 \vspace{5mm} %5mm vertical space
 
 \textbf{Keywords}: Bitcoin; trading strategy; crypotocurrency; neural networks; Forecasting; Economic and Financial Time Series; Deep Learning; Long Short-Term Memory (LSTM); Autoregressive Integrated Moving Average (ARIMA); Root Mean Squared Error (RMSE)



\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}

\section{Introduction}
There are many research papers nowadays investigating which forecasting methods offer best predictions with respect to lower forecast errors and higher accuracy of forecasts. Traditional time series prediction methods include univariate Autoregressive (AR), univariate Moving Average (MA), Simple Exponential Smoothing (SES), and Autoregressive Integrated Moving Average (ARIMA). Due to lack of seasonality in the stock markets and its high volatility, these traditional linear statistical model are not very effective for this task. Machine learning methods seem to be promising in this regard, since they can incorporate non-linearity in prediction models to deal with non-stationary financial time-series. 

Particularly, Siami-Namini's paper has successfully applied Machine Learning techniques called  LSTM model for stock markets prediction: it achieves 84-87 percent average reduction in error rate when compared to ARIMA, indicating the superiority of LSTM to ARIMA. Without doubt, expecting improvement is reasonable as RNN and LSTM neural networks is likely to predict prices better than the traditional multi-layer perceptron (MLP) and linear algorithm like ARIMA due to the temporal nature of the more advanced algorithms. However, although the result from this paper seems promising, there is a dearth of research critique and replication to validate such result. Therefore, the main objective of this article is to test and verify the empirical result of their findings.

In our paper, we focus on two aspects to verify the validity of the paper. First, although the paper shows significant improvement in predictive power using the deep learning model, the authors of the paper do not make relevant data publicly available for the readers to replicate its result. As the movement to examine and enhance the reliability of research expands, the lack of methodology and analysis plan in the paper make us feel that such a result might suffer from p-hacking, since the authors would have the freedom to make choices such as  which observations to measure, which variables to compare, which factors to combine, and which ones to control for. As a result, such a freedom would allow the authors to manipulate their study conditions until they get the answer that they want. Therefore, in order to verify the conclusion of the paper, we consider a set of different data to train the models and compare their predictive performance with each other. Secondly, the growth of virtual currencies has fueled interest from the scientific community. As cryptocurrencies have faced period rise and sudden dips in specific time periods, cryptocurrency trading community has a need for a standardized method to accurately predict the fluctuating price trends. Therefore, for application purposes in algorithmic trading, we use high-frequency Bitcoin data to implement our proposed architecture to test two two simple trading strategies, and then evaluate the merit of paper's claim from a trading strategy perspective.

We will split this paper into two parts:
\begin{enumerate}
  \item Evaluation of paper's methodology and results
  \item Implementation of alternative specifications for Bitcoin (BTC) data.
\end{enumerate}

\section{Evaluation of Paper's Methodology}

\subsection{Data analysis}
\label{sec:headings}

\subsubsection{Selection of Dataset}
As mentioned above, the dataset used by the paper, though stated, is not clearly described. In particular, they used only 200 data points, although it is not stated what time period these were selected from or why. Therefore, we test the model on predicting the price of other common financial products. We chose the State Street S\&P 500 Index ETF (NYSE: SPY) as the time series to be predicted. We chose this series as it is a good proxy for the overall market and exhibits minimal tracking error with the S&P 500 Index. It is also heavily traded and thus trades very close to its NAV.

\subsubsection{Data Features}

The dataset contains adjusted closing prices of SPY from January 2000 to August 2019, which encompasses about 4940 data points. We split this into 3740 training samples and 1200 test samples.

Similar to the paper, we found the data to be non-stationary. First-differencing the data yielded a stationary time-series, hence justifying the paper's use of 1 integration term in the ARIMA specification. This is also in line with the prevailing theory about stock market returns. There does appear to be some amount of heteroskedasticity in the returns, however we ignore this for now. 

\begin{figure}
  \centering
  \includegraphics{SPY Raw Series.png}
  \caption{State Street S\&P 500 Index (Adjusted Close)}
\end{figure}

\FloatBarrier

\begin{figure}
  \centering
  \includegraphics{SPY Differenced.png}
  \caption{State Street S\&P 500 Index First Differenced}
\end{figure}

\FloatBarrier

\subsection{Architecture}

The paper used an ARIMA(5,1,0) as a baseline model for comparison. It is not explained in the paper why 5 lags are used, not are confidence intervals clearly stated. 

As an improvement over this model, the paper proposes an LSTM model with 1 Dense Layer and an Adam Optimizer using a Mean Square Error loss. The architecture of the model is summarized by the following code:

\begin{verbatim}
model = Sequential()
model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))
model.add(Dense(1))
optimizer = optimizers.Adam(clipvalue=0.5)
model.compile(loss='mean_squared_error', optimizer=optimizer)
\end{verbatim}

A clipvalue parameter was added by us to avoid a vanishing gradient problem during the training.

\subsection{Training}

% \includegraphics{Training Progress.PNG}
% \caption{Evolution of training loss}

\begin{figure}[h]
\centering
\includegraphics{Training Progress.PNG}
\caption{Evolution of training loss}
\end{figure}

The training process uses 10 epochs. It is quite fast and is completed in 5-10 seconds at most. 

After running the training process we observed a few concerning problems. Firstly, the training loss plateaued after the first epoch, indicating that the deep-learning model isn't actually learning much from the training data. Secondly, the paper appears to be aware of this issue however, it makes no comment about it. 

\subsection{Tests: Daily SPY Price and Returns}

Next, we use the LSTM model to perform rolling forecasts. This is done by looking at today's actual SPY level to predict tomorrow's level. A similar strategy is used for the ARIMA(5,1,0) model.

Firstly, to evaluate the paper's claim of an improvement in RMSE for the level prediction we predicted over the entire test horizon.

\begin{figure}[h]

\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{LSTM Prediction.PNG} 
\caption{LSTM Predictions}
\label{fig:subim1}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{ARIMA Prediction.PNG}
\caption{ARIMA Predictions}
\label{fig:subim2}
\end{subfigure}

\caption{Rolling Predictions of SPY Levels}
\label{fig:image2}
\end{figure}

As is evident from the graphs, the two models are pretty much identical in their performance. Quantitatively, LSTM produced an RMSE of 1.932 vs 1.937 for ARIMA. We do not believe this is a significant or meaningful improvement. This calls into question the paper's claims of an 85\% improvement in the RMSE.

Next, to predict the returns we compute the percentage change between the observed price today and the predicted price tomorrow.

\begin{figure}[h]

\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{LSTM Returns.PNG} 
\caption{LSTM Returns}
\label{fig:subim1}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{ARIMA Returns.PNG}
\caption{ARIMA Returns}
\label{fig:subim2}
\end{subfigure}

\caption{Rolling Predictions of Returns of SPY}
\label{fig:image2}
\end{figure}

Clearly the return is indicative that there is very little to no predictability in the returns. 

\subsection{Final Rebuttal: The Naive Model}

As a final rebuttal, we constructed what we call the Naive Model. This model simply predicts tomorrow's SPY level to be the same as today. For example if the level was \$300 today, it will be predicted as \$300 tomorrow. A consequence of this is that the return prediction is always 0 (martingale property).

We evaluated the RMSE Naive Model and found it to be 1.934. We believe this is a final blow to the paper's claims of superiority of the LSTM model as we can see it performs no better than a very simplistic model. 

In hindsight this is not surprising. Predicting returns is very difficult and it would be indeed surprising if they could be predicted with any degree of accuracy using lagged returns as the only explanatory variable. 

\section{Evaluation of Paper's Methodology}

% \subsection{Trading strategy: 1-min BTC/USD}
% Secondly, when constructing the trading strategy, we improved the model by using high-frequency data, i.e. 1-min frequency BTC price during 2014/12/01 – 2019/01/09. The dataset was downloaded from kaggle\footnote{www.kaggle.com/mczielinski/bitcoin-historical-data}, with basic information of the price: open, close, high, low, volume as shown in figure 1.

\subsection{Feature Engineering}
Data for the present study has been collected from several sources. We have selected features that may be driving Bitcoin prices and have performed feature engineering to obtain independent variables for future price prediction. Bitcoin prices are driven by a combination of various endogenous and exogenous factors (Elie Bouri, 2017). The inherent technical aspects of Bitcoin are considered to be endogenous factors. 1-min Bitcoin times series data in USD is obtained from Coinbase, while 1-min Etherem dataset is obtained from Kaggle. After cleaning and manipulating the datasets and doing feature engineering, the key endogenous features considered in the present study are Bitcoin price, Bitcoin 10-min return, relative 10-min Bitcoin trading volume, 10-min high-minus-low price over mean close price, log realized volatility, 10min mean price minus 1-hour mean price, 2-hour mean price minus 12-hour mean price.  The exogenous features consist of second largest cryptocurrency, Ethereum, which may impact the prices. The exogenous factor of interest in this study is standardized Ethereum price . Ethereum price is used is to investigate the characteristics of Bitcoin investors and price relationship between the two largest cryptocurrencies. Some technical analysis variables are constructed to explore how these technical analysis indicators can predict future Bitcoin prices. We provide the definitions of these features and data sources in \textbf{Appendix I}.

\begin{figure}[h]
  \centering
  \includegraphics{BTc_data.png}
  \caption{Sample BTC price data.}
  \label{fig:fig1}
\end{figure}




\subsection{Baseline}
\subsection{}



\subsection{}


\subsection{Architecture}



\subsection{Trading Model}

\subsubsection{Introduction}

With a promising model in place, we now focus on implementing a trading strategy to exploit the model's predictions. There are several practical considerations to be made when implementing a trading strategy than can simulate a real world trading environment. These are as follows:

\begin{enumerate}
  \item Trade signal has a lot of jitter. In a practical environment, we would not want to trade excessively on every period's signal.
  \item Because there are always trading costs involved, we may want to restrict our trading to signals that are stronger than a chosen threshold. 
  \item Finally, we may want some asymmetry in our trading strategy i.e. we may want to liquidate open positions much more aggressively (in response to trade signals) than opening new positions (or vice versa).
  \item We need a model of trading costs for penalizing excessive trading
\end{enumerate}

To mitigate these issues, we implement the following solutions:

\begin{enumerate}
  \item We use Exponential Moving Average of return signals to determine trade signals. The length of the averaging period will be a hyperparameter.
  \item We only trade if the return signal is larger than some fraction of last day’s volatility. The fraction to be used will be a hyperparameter.
  \item We use different signal thresholds for entering and exiting positions.
  \item We use a proportional trading cost of 0.15\% per trade.
\end{enumerate}

\subsubsection{Initial Results}

For an initial run of the trading model, we made an arbitrary choice of hyperparameters. We open a long (short) a position if the predicted return is greater (lesser) than 1 times the 1 day volatility. We close a long (short) position if the return is lesser (greater) than 0.1 times the 1 day volatility. 

After running this strategy on the test set, we observe that the model traded 118 times and produced a Sharpe Ratio of 0.662 for a return of 73\%.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth, height=5cm]{Portfolio Return.png}
  \caption{Portfolio Return on test set}
  \label{fig:fig1}
\end{figure}

This looks very good as BTC only returned 50\% over the same period. However, there is a catch. The return is highly hyperparameter dependent and our hyperparameters were chosen arbitrarily.

\subsubsection{Hyperparameter Tuning}

After running a cross-validation approach, we found that the trading strategy is highly dependent on the chosen levels of trading aggressiveness (the open/close constants). Moreover the hyperparameters are highly unstable over time and sometimes lead to highly negative returns.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth, height=5cm]{Hyperparameter Tuning.png}
  \caption{Portfolio Return on test set}
  \label{fig:fig1}
\end{figure}

In addition to the explicit hyper parameters, there are several implicit ones that we did not tune. For example: 

\begin{enumerate}
  \item Length of Recalibration interval (30 days in above results).
  \item Length of Retraining interval (90 days right now).
  \item Finer grid of open/close constants.
\end{enumerate}

\subsubsection{Limitations}

Our model is inherently very short term. It is only meant to predict returns over very short horizons. Thus it is not surprising that while it performs well on short term metrics its performance deteriorates over time. 

Moreover, the trading costs exact a heavy toll on our model. Every trade costs us 0.15\% and thus the 118 trades in our previous results cost as much as 18\% of the return.

\subsubsection{Refinements}

There are several refinements possible to consider:

\begin{enumerate}
  \item Take into account bid ask spreads when computing returns.
  \item Execution delay – In practice we may not be able to trade instantly on receiving signal.
  \item Determine correct static rebalancing interval and hyperparameters – Need more computational power.
  \item Better still – Model should recognize persistent underperformance on its own and signal the trading platform as such – Dynamic retraining intervals.
\end{enumerate}

\section{Conclusion}
With the recent advancement on developing sophisticated machine learning-based techniques and deep learning algorithms, many research papers have claimed that their empirical findings have shown the superior performance of machine learning and deep learning models over the traditional time-series model in predicting the prices of various financial assets. The major question is then how convincing and reliable these papers' result are. This paper focus on one such empirical finding by applying different financial data set to compare the predicting accuracy of ARIMA and LSTM and then constructing high-frequency trading strategy of Bitcoin to test profitability of LSTM model. These two predicting techniques were implemented and applied on State Street S\&P 500 Index ETF (NYSE: SPY) and the result indicated that RMSEs of these two techniques are essentially the same. More specifically, the LSTM-based and ARIMA  algorithms have values of 1.932 and 1.937 respectively. Furthermore, taking into account of the transaction cost, trading strategy based on the LSTM algorithms do not generate significant risk-adjusted return. The work described in this paper casts doubt on the benefits of applying deep learning-based algorithms and techniques to the economics and financial data. 
 
However, there are further work to refine our deep learning models and potentially improve their predicting power. For instance, deep learning models require accurate training and hyperparameter tuning to yield results. Although training might be computationally extensive for large datasets unlike conventional time-series approaches, the market data is always limited and computational complexity is not really a big concern for stock or cryptocurrency price prediction, and thus shallow learning quantitative finance in the coming years. Therefore, with proper back testing of these deep learning model are likely contribute significantly to manage portfolio risk and reduce financial losses. Nonetheless a significantly improvement over the current study is likely achieved if a bigger data set is available.





\section{Recommendation for production}


\bibliographystyle{unsrt}  
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
\begin{thebibliography}{1}

\bibitem{kour2014real}
Sima Siami Namini, Akbar Siami Namin.
\newblock Forecasting Economics and Financial Time Series: ARIMA vs. LSTM. 
\newblock In {\em 	arXiv:1803.06386}, 2018.

\bibitem{kour2014fast}
Bao W, Yue J, Rao Y.
\newblock A deep learning framework for financial time series using stacked autoencoders and long-short term memory.
\newblock In {\em PLoS ONE 12(7): e0180944}, 2017.

\bibitem{hadash2018estimate}
Aniruddha Dutta, Saket Kumar and Meheli Basu.
\newblock A Gated Recurrent Unit Approach to Bitcoin Price Prediction. 
\newblock {\em arXiv preprint 	arXiv:1912.11166}, 2019.

\bibitem{hadash2018estimate}
Bouri, E, Molnár, P, Azzi, G, Roubaud, D.
\newblock On the hedge and safe haven properties of Bitcoin: Is it really more than a diversifier?
\newblock {\emFinance Research Letters 20: 192-198}, 2017.

\end{thebibliography}


\end{document}
